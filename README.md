<h1>
  <img src="assets/logo.png" alt="MitoChat" width="50" style="vertical-align: middle;"/>
  MitoChat: RAG + Agentic Streamlit Application
</h1>

**MitoChat** is a Retrieval-Augmented Generation (RAG) application designed to support genomic and clinical knowledge exploration using a GPU-accelerated backend.  

The stack includes:

- A **Streamlit web interface** (frontend)
- A **FastAPI RAG backend** powered by:
  - **vLLM** (GPU-accelerated text generation)
  - **FAISS** (dense retrieval)
  - **MiniLM** (embeddings)
  - **BGE-reranker-base** (cross-encoder reranking)
  - **Helsinki MarianMT** (FR â†” EN translation)
  - **spaCy** (optional, for sentence-level highlighting)
- A modular codebase to later add STT/TTS microservices (Whisper, Kokoro, etc.).

---

## ğŸ“ Repository Structure

```text
.
â”œâ”€â”€ assets/                     
â”œâ”€â”€ data/                       
â”‚   â”œâ”€â”€ index/
â”‚   â”œâ”€â”€ clinvar/
â”‚   â”œâ”€â”€ genereviews/
â”‚   â””â”€â”€ mitocarta/
â”‚
â”œâ”€â”€ models/                     
â”‚   â”œâ”€â”€ sentence-transformers/all-MiniLM-L6-v2/
â”‚   â”œâ”€â”€ BAAI/bge-reranker-base/
â”‚   â”œâ”€â”€ Helsinki-NLP/opus-mt-fr-en/
â”‚   â””â”€â”€ Helsinki-NLP/opus-mt-en-fr/
â”‚
â”œâ”€â”€ prompts/                    
â”‚   â”œâ”€â”€ rewrite_en.yaml
â”‚   â””â”€â”€ rewrite_fr.yaml
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ build_corpus_and_index_prod.py
â”‚   â”œâ”€â”€ rag_core.py
â”‚   â”œâ”€â”€ fastapi_backend.py
â”‚   â”œâ”€â”€ streamlit_app_frontend.py
â”‚   â”œâ”€â”€ pdf_rendering.py
â”‚   â”œâ”€â”€ utils.py
â”‚   â””â”€â”€ config.yaml
â”‚
â”œâ”€â”€ docker/
â”‚   â”œâ”€â”€ backend.Dockerfile
â”‚   â”œâ”€â”€ frontend.Dockerfile
â”‚
â”œâ”€â”€ requirements.backend.txt
â”œâ”€â”€ requirements.frontend.txt
â”œâ”€â”€ docker-compose.yml
â””â”€â”€ README.md
```

---

## âš™ï¸ Features

### Backend (FastAPI + RAG)

- French query âœ **FRâ†’EN translation** âœ RAG in English âœ **ENâ†’FR translation**
- **FAISS** dense retrieval
- **MiniLM** embeddings 
- **BGE-reranker-base** reranking
- **vLLM** LLM serving (GPU)
- Optional **spaCy** integration for:
  - sentence splitting,
  - highlighting the most relevant sentences inside retrieved chunks.

### Frontend (Streamlit)

- Chat UI
- Sends French queries, receives French answers

---

## ğŸ› ï¸ 1. Local Development (without Docker)

### Create & activate a virtual env

```bash
python -m venv venv_clean

# Windows
venv_clean\Scripts\activate

# Linux / macOS
source venv_clean/bin/activate

```

### Install deps

```bash
pip install -r requirements.backend.txt
pip install -r requirements.frontend.txt
```

If you want to run vLLM locally outside Docker, you also need:

```bash
pip install vllm
```

---

## ğŸ§± 2. Build FAISS Index

The backend expects a FAISS index and docstore.

```bash
python scripts/build_corpus_and_index_prod.py
```

This will:
- Load documents from data/clinvar, data/genereviews, data/mitocarta, etc.
- Encode them with MiniLM / sentence-transformers
- Build a FAISS index
- Store metadata in `data/index/` and associated files.

---

## ğŸš€ 3. Run Backend & Frontend Locally

Run backend (FastAPI + uvicorn):

```bash
uvicorn scripts.fastapi_backend:app --host 0.0.0.0 --port 9000 --reload
```

Open the interactive docs: http://localhost:9000/docs

Run frontend (Streamlit):

```bash
streamlit run scripts/streamlit_app_frontend.py
```

Access the UI: http://localhost:8501

---

## ğŸ³ 4. Docker Setup

This project uses **two Docker images**:
- `backend.Dockerfile`: FastAPI + vLLM + RAG (GPU)
- `frontend.Dockerfile`: Streamlit UI (CPU)


### 4.1 Backend Dockerfile (overview)

The backend image:

- Uses a **CUDA 12.1** runtime base image  
- Installs **Python**, **PyTorch (cu121)**, **vLLM**  
- Installs all backend dependencies from `requirements.backend.txt`  
- Copies backend code, scripts, and prompts  
- **Does NOT include models or data** â†’ they are mounted as **volumes**  
- Exposes **port 9000**  
- Launches FastAPI via:

```bash
uvicorn scripts.fastapi_backend:app --host 0.0.0.0 --port 9000
```

This image is GPU-enabled and requires:

- Host NVIDIA drivers  
- NVIDIA Container Toolkit  
- `--gpus all` (compose) or device reservation  


### 4.2 Frontend Dockerfile (overview)

The frontend image:

- Uses **python:3.11-slim**  
- Installs dependencies via `requirements.frontend.txt`  
- Copies `scripts/` and `assets/`  
- Exposes **port 8501**  
- Runs Streamlit:

```bash
streamlit run scripts/streamlit_app_frontend.py --server.address=0.0.0.0
```

The frontend container has **no GPU requirements**.


### 4.3 Building Docker Images

From the project root:

### Backend:

```bash
docker build -f docker/backend.Dockerfile -t mitochat-backend .
```

### Frontend:

```bash
docker build -f docker/frontend.Dockerfile -t mitochat-frontend .
```

---

## ğŸ§© 5. Docker Compose Deployment

```bash
docker compose up -d --build
```

This starts:

- **Backend** at port **9000**  
- **Frontend** at port **8501**  

Check logs:

```bash
docker compose logs -f backend
docker compose logs -f frontend
```

---

## ğŸ§  6. GPU / vLLM Notes

The backend image includes:

- PyTorch + CUDA 12.1  
- `vllm`  

Requirements on host:

- NVIDIA driver  
- NVIDIA Docker Toolkit  
- `docker run --gpus all ...`  

Check GPU access:

```bash
docker exec -it mitochat_backend python3 -c "import torch; print(torch.cuda.is_available())"
```

---

## ğŸŒ 7. FastAPI Endpoints

Open API documentation: http://localhost:9000/docs

Example request:

```bash
curl -X POST http://localhost:9000/rag/query   -H "Content-Type: application/json"   -d '{"query": "Que sais-tu sur MT-ND1 ?"}'
```

---

## ğŸ–¥ï¸ 8. Production Architecture (Example)

A target setup might look like:

```text
Internet Users
    |
    |  TCP 443 (HTTPS)
    v
   WAF
    |
    |  TCP 443 (HTTPS)
    v
[ DMZ SERVER ]
    â”œâ”€ Reverse Proxy (nginx/caddy)
    â”‚      â””â”€ forwards to Streamlit UI (localhost:8501)
    â”‚
    â””â”€ Streamlit Web App (Frontend UI)
           â”œâ”€ Displays chat interface
           â”œâ”€ Sends raw user query (FR) â†’ RAG API
           â””â”€ Receives final FR answer

[ GPU SERVER ]
    â”œâ”€ RAG Backend API (FastAPI + vLLM)
    â”‚     â”œâ”€ Translation (FRâ†”EN)
    â”‚     â”œâ”€ RAG core (FAISS + embeddings + reranker)
    â”‚     â””â”€ LLM generation with vLLM
    â”‚
    â”œâ”€ Local assets:
    â”‚     â”œâ”€ Models (translation, embeddings, reranker, LLM)
    â”‚     â””â”€ FAISS index & docstore (mounted under /app/data)

```

## ğŸ“„ License

MIT License.
