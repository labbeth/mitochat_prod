paths:
  # Accept file paths, directories, or glob patterns (strings or lists)
  logo: assets/logo.png

  genereviews_inputs:
    - ../data/genereviews
    - ../data/genereviews/*.pdf
    - ../data/genereviews/*.txt
  clinvar_inputs:
    - ../data/clinvar
    - ../data/clinvar/*.json
  mitocarta_inputs:
    - ../data/mitocarta
    - ../data/mitocarta/*.json

  # Legacy fallbacks (optional; used if the inputs above are empty)
  pdf_path: ../data/genereviews/Bookshelf_NBK1173.pdf
  variants_path: ../data/json_samples/clinvar_mito_sample.json
  genes_path: ../data/json_samples/mitocarta_pivot_sample.json

#  index_dir: index
  index_dir: data/index
  outputs_dir: ../outputs

ingestion:
  use_unstructured: true

  # Smaller, more granular chunks for narrative PDFs/TXTs
  chunk_tokens_pdf: 450
  chunk_overlap_pdf: 80

  # Default chunking (used for everything else unless overridden)
  chunk_tokens: 700
  chunk_overlap: 100

embedding:
  model_name: models/sentence-transformers/all-MiniLM-L6-v2  # ../models/sentence-transformers/all-MiniLM-L6-v2
  batch_size: 64
  normalize: true

indexing:
  # Build per-source indices to enable multi-stage retrieval (pdf / clinvar / mitocarta)
  build_subindices: true
  # Save identifier maps for fast exact lookups (variant_id, rsid, gene_symbol)
  save_id_maps: true

retrieval:
  # Unified dense search knobs (also used as fallback when subindices not present)
  top_k: 25
  enable_reranker: true
  rerank_top_k: 8
  reranker_model: models/BAAI/bge-reranker-base

  # Multi-stage: search each source independently → merge → rerank → rebalance
  multi_stage: true
  per_source_k:
    genereviews: 20
    clinvar: 6
    mitocarta: 6
  final_k: 8

  # Source-aware balancing
  source_boosts:
    genereviews: 0.25
    # New source names (preferred)
    clinvar: -0.20
    mitocarta: -0.10
    # Backward-compat keys (safe to keep; ignored if unused)
    pdf: 0.25
    json_variants: -0.20
    json_genes: -0.10

  per_source_cap:
    genereviews_min: 4
    # New source names (preferred)
    clinvar_max: 2
    mitocarta_max: 2
    # Backward-compat keys
    pdf_min: 4
    json_variants_max: 2
    json_genes_max: 2

  # Prefer narrative PDFs for guidance/treatment/surveillance questions
  enforce_pdf_for_guidance: true
  guidance_keywords:
    - "frequency"
    - "surveillance"
    - "monitor"
    - "monitoring"
    - "follow-up"
    - "follow up"
    - "management"
    - "treatment"
    - "avoid"
    - "contraindicated"
    - "recommend"
    - "recommended"
    - "EKG"
    - "ECG"
    - "echocardiogram"
    - "Holter"
    - "blood pressure"
    - "annually"
    - "every 6"

  # Small lexical bump on PDFs for key guidance terms
  hybrid_lexical: true
  lexical_weight_pdf: 0.02

  # Structured exact-ID lookup fast-path (before dense retrieval)
  structured_lookup:
    enabled: true
    allow:
      - variant_id
      - rsid
      - gene_symbol

generation:
#  provider: llama_cpp
  model_path: ../models/Qwen2.5-7B-Instruct-Uncensored.Q4_K_M.gguf
  provider: vllm
  vllm_base_url: "http://127.0.0.1:8000/v1"
  vllm_model: "qwen2.5-7b-instruct"
  vllm_api_key: "EMPTY"
  temperature: 0.2
  max_tokens: 400
  num_ctx: 4096
  n_gpu_layers: 20      # 0 = CPU only; -1 = GPU
  n_threads: 8         # or your CPU core count
  n_batch: 256
  system_prompt: >
    You are a helpful medical assistant. Respond ONLY with the information provided in the context.
    If the information is not present, say that you do not know. Add citations in square brackets [1], [2] corresponding to the listed sources.

ner:
  model: en_core_sci_sm   # or en_core_web_sm

audio:
  enable_voice: true

  STT:
    backend: "faster-whisper"   # "faster-whisper" | "vosk"
    model: "base"               # tiny/base/small ... (or "tiny.en" if only EN)
    compute_type: "int8"        # smallest and fast on CPU
    vad_filter: true
    language: "fr"              # force "fr" (or null for auto)
    vosk_model_dir: "../models/vosk"        # set if backend == "vosk"

  TTS:
    backend: "kokoro_kpipeline"           # "kokoro" | "pyttsx3"
    model_dir: "../models/kokoro"  # contains kokoro-v1.onnx + voices/
    lang_code: "f"
    voice: "ff_siwis"
    speed: 1.0
    output_format: "wav"

translation:
  provider: "marian"  # (deepl, argos, etc.)
#  model_name: "../models/facebook/m2m100_418M"
#  fr_en_model: ../models/Helsinki-NLP/opus-mt-fr-en
#  en_fr_model: ../models/Helsinki-NLP/opus-mt-en-fr
  fr_en_model: models/Helsinki-NLP/opus-mt-fr-en
  en_fr_model: models/Helsinki-NLP/opus-mt-en-fr
  device: null     # "cuda" | "cpu" | null (auto-pick)
  max_new_tokens: 512

pdf_metadata:
  source_url: "https://www.ncbi.nlm.nih.gov/books/"
  license_note: "© 1993-2025 University of Washington (GeneReviews) — noncommercial research use with attribution"

advanced:
  # Allow overriding the config path via CONFIG env var (app will search sensible defaults too)
  allow_env_override: true

evaluation:
  annotation_files:
    - ../data/evaluation/questions_annotations_clinvar.json
    - ../data/evaluation/questions_annotations_mitocarta.json
    - ../data/evaluation/questions_annotations_genereviews.json


#paths:
#  # Accept file paths, directories, or glob patterns (strings or lists)
#  genereviews_inputs:
#    - ../data/genereviews                    # a directory (recurses 1 level)
#    - ../data/genereviews/*.pdf              # glob pattern
#  clinvar_inputs:
#    - ../data/clinvar
#    - ../data/clinvar/*.json
#  mitocarta_inputs:
#    - ../data/mitocarta
#    - ../data/mitocarta/*.json
#  pdf_path: ../data/genereviews/Bookshelf_NBK1173.pdf
#  variants_path: ../data/json_samples/clinvar_mito_sample.json
#  genes_path: ../data/json_samples/mitocarta_pivot_sample.json
#  index_dir: index
#
#ingestion:
#  use_unstructured: true                # if false or unstructured not installed, falls back to PyMuPDF or plain text
#  chunk_tokens: 700
#  chunk_overlap: 100
#
#embedding:
#  model_name: sentence-transformers/all-MiniLM-L6-v2
#  batch_size: 64
#  normalize: true
#
#retrieval:
#  top_k: 20
#  enable_reranker: true
#  rerank_top_k: 5
#  reranker_model: BAAI/bge-reranker-base
#
## New knobs
#  source_boosts:
#    pdf: 0.25
#    json_variants: -0.20
#    json_genes: -0.10
#
#  per_source_cap:
#    pdf_min: 3
#    json_variants_max: 2
#    json_genes_max: 2
#
#  enforce_pdf_for_guidance: true
#  guidance_keywords: ["frequency", "surveillance", "monitor", "monitoring", "follow-up", "follow up",
#                      "management", "treatment", "avoid", "contraindicated", "recommend", "recommended",
#                      "EKG", "ECG", "echocardiogram", "Holter", "blood pressure"]
#
#  hybrid_lexical: true
#  lexical_weight_pdf: 0.02     # per keyword match, small additive bump on PDFs
#
#generation:
#  provider: llama_cpp
#  model_path: ../models/qwen2.5-3b-instruct-q4_k_m.gguf   # or the 1.5B path
#  temperature: 0.2
#  max_tokens: 400
#  num_ctx: 4096
#  n_gpu_layers: 0       # CPU only
#  n_threads: 8          # or your CPU core count
#  n_batch: 256
#  system_prompt: >
#    You are a helpful FRENCH medical assistant. Answer using ONLY the provided context. If the
#    answer is not in the context, say you don’t know. Include bracketed
#    citations like [1], [2] that refer to the sources list provided.
#
#pdf_metadata:
#  source_url: "https://www.ncbi.nlm.nih.gov/books/"
#  license_note: "© 1993-2025 University of Washington (GeneReviews) — noncommercial research use with attribution"
#
#advanced:
#  # Optionally override config path via environment variable CONFIG
#  # Leave empty if not needed.
#  allow_env_override: true
